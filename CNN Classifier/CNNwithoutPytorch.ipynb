{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import struct\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        magic, num_images, num_rows, num_cols = struct.unpack('>IIII', f.read(16))\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        \n",
    "        images = data.reshape(num_images, 1, num_rows, num_cols).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        magic, num_items = struct.unpack('>II', f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        \n",
    "        return labels\n",
    "\n",
    "# X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "# Y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "\n",
    "# print(f\"Training Image Shape: {X_train.shape}\")\n",
    "# print(f\"Training Label Shape: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e5fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"Our simple non-linear activation function.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Converts raw scores (logits) into probabilities that sum to 1.\"\"\"\n",
    "    # To prevent overflow during exponentiation, subtract the max value\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def cross_entropy_loss(probabilities, true_labels):\n",
    "    \"\"\"\n",
    "    Calculates the Cross-Entropy Loss (error) for the current batch.\n",
    "    \n",
    "    Args:\n",
    "        probabilities: (Batch_Size, 10) array of predicted probabilities.\n",
    "        true_labels: (Batch_Size,) array of integer labels (0-9).\n",
    "    \"\"\"\n",
    "    m = true_labels.shape[0] # Batch size\n",
    "    \n",
    "    # The one-hot encoding is not strictly needed for the loss calculation \n",
    "    # but is useful for understanding:\n",
    "    # one_hot_labels = np.zeros((m, 10))\n",
    "    # one_hot_labels[np.arange(m), true_labels] = 1\n",
    "\n",
    "    # Get the predicted probability for the *correct* class for each image in the batch.\n",
    "    # This uses the 1D 'true_labels' array to index the probabilities array.\n",
    "    # The typo 'true_hot_labels' has been fixed to 'true_labels'.\n",
    "    log_probs = -np.log(probabilities[np.arange(m), true_labels] + 1e-9) \n",
    "    \n",
    "    # Return the average loss\n",
    "    return np.sum(log_probs) / m\n",
    "\n",
    "def manual_conv2d(image, filters, bias, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Manually performs the 2D convolution operation (Forward Pass).\n",
    "    Note: Simplified for single-image batch, no padding implemented here.\n",
    "    \"\"\"\n",
    "    C_out, C_in, F_h, F_w = filters.shape\n",
    "    C_in, H_in, W_in = image.shape\n",
    "\n",
    "    # Calculate Output Dimensions: H_out = (I - F + 2P)/S + 1\n",
    "    H_out = (H_in - F_h + 2 * padding) // stride + 1\n",
    "    W_out = (W_in - F_w + 2 * padding) // stride + 1\n",
    "    \n",
    "    output = np.zeros((C_out, H_out, W_out), dtype=np.float32)\n",
    "\n",
    "    for k in range(C_out):   # Loop over 5 Filters\n",
    "        for i in range(H_out): # Loop over Output Height (e.g., 26 times)\n",
    "            for j in range(W_out): # Loop over Output Width (e.g., 26 times)\n",
    "                \n",
    "                # Define the slice of the input image that the filter sees\n",
    "                h_start, h_end = i * stride, i * stride + F_h\n",
    "                w_start, w_end = j * stride, j * stride + F_w\n",
    "                input_patch = image[:, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                # Core Operation: Sum(Multiplication) + Bias\n",
    "                output[k, i, j] = np.sum(input_patch * filters[k]) + bias[k]\n",
    "\n",
    "    return output\n",
    "\n",
    "def manual_max_pool(image, pool_size=2, stride=2):\n",
    "    \"\"\"Manually performs the Max Pooling operation.\"\"\"\n",
    "    C, H_in, W_in = image.shape\n",
    "    \n",
    "    # Calculate Output Dimensions: H_out = I / S (for P=0, F=S)\n",
    "    H_out = H_in // stride\n",
    "    W_out = W_in // stride\n",
    "    \n",
    "    output = np.zeros((C, H_out, W_out), dtype=np.float32)\n",
    "\n",
    "    for c in range(C):\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                \n",
    "                h_start, h_end = i * stride, i * stride + pool_size\n",
    "                w_start, w_end = j * stride, j * stride + pool_size\n",
    "                \n",
    "                input_patch = image[c, h_start:h_end, w_start:w_end]\n",
    "                \n",
    "                # Core Operation: Find the Maximum value\n",
    "                output[c, i, j] = np.max(input_patch)\n",
    "                \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    \"\"\"Initializes all weights and biases (W1, W2, W3, W4) with small random values.\"\"\"\n",
    "    # We use a simple small uniform distribution for initialization\n",
    "    def initialize(shape):\n",
    "        # Using Xavier/Glorot initiation for a slightly better start\n",
    "        limit = np.sqrt(6 / (shape[0] + shape[1]))\n",
    "        return np.random.uniform(-limit, limit, size=shape).astype(np.float32)\n",
    "\n",
    "    params = {}\n",
    "    # Layer 1: Conv (5 filters, 1 channel in, 3x3 kernel)\n",
    "    params['W1'] = initialize((5, 1, 3, 3))\n",
    "    params['b1'] = np.zeros(5, dtype=np.float32)\n",
    "    \n",
    "    # Layer 2: Conv (10 filters, 5 channels in, 3x3 kernel)\n",
    "    params['W2'] = initialize((10, 5, 3, 3))\n",
    "    params['b2'] = np.zeros(10, dtype=np.float32)\n",
    "\n",
    "    # Layer 3: FC (Input 250 features, Output 100 neurons)\n",
    "    params['W3'] = initialize((100, 250)) \n",
    "    params['b3'] = np.zeros(100, dtype=np.float32)\n",
    "\n",
    "    # Layer 4: FC (Output 10 classes)\n",
    "    params['W4'] = initialize((10, 100))\n",
    "    params['b4'] = np.zeros(10, dtype=np.float32)\n",
    "\n",
    "    return params\n",
    "\n",
    "def forward_pass(image, params):\n",
    "    \"\"\"Defines the full architecture: Conv1 -> Pool1 -> Conv2 -> Pool2 -> FC.\"\"\"\n",
    "    \n",
    "    # Layer 1: Conv -> ReLU -> Pool\n",
    "    conv1_output = manual_conv2d(image, params['W1'], params['b1'])\n",
    "    relu1_output = relu(conv1_output)\n",
    "    pool1_output = manual_max_pool(relu1_output) # Output: 5 x 13 x 13\n",
    "\n",
    "    # Layer 2: Conv -> ReLU -> Pool\n",
    "    conv2_output = manual_conv2d(pool1_output, params['W2'], params['b2'])\n",
    "    relu2_output = relu(conv2_output)\n",
    "    pool2_output = manual_max_pool(relu2_output) # Output: 10 x 5 x 5\n",
    "\n",
    "    # Layer 3: Flatten (10 x 5 x 5 = 250 features)\n",
    "    flattened = pool2_output.flatten() # Vector of 250 features\n",
    "\n",
    "    # Layer 4: FC 1 -> ReLU\n",
    "    fc1_output = np.dot(params['W3'], flattened) + params['b3']\n",
    "    relu3_output = relu(fc1_output)\n",
    "\n",
    "    # Layer 5: FC 2 (Output 10 neurons)\n",
    "    fc2_output = np.dot(params['W4'], relu3_output) + params['b4']\n",
    "    \n",
    "    # Final Activation: Softmax to get probabilities (p)\n",
    "    probabilities = softmax(fc2_output)\n",
    "    \n",
    "    # Storing outputs for the Backward Pass (Chain Rule)\n",
    "    # The dictionary no longer uses the invalid '...' placeholder.\n",
    "    return probabilities, {\n",
    "        'relu1_output': relu1_output, \n",
    "        'relu2_output': relu2_output,\n",
    "        'relu3_output': relu3_output,\n",
    "        'flattened': flattened, \n",
    "        'pool1_output': pool1_output,\n",
    "        'pool2_output': pool2_output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c238d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_mnist_images('MNIST/raw/train-images-idx3-ubyte.gz')\n",
    "Y_train = load_mnist_labels('MNIST/raw/train-labels-idx1-ubyte.gz')\n",
    "\n",
    "params = init_params()\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "print(\"Starting manual training loop...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # For each epoch, iterate through the training data in batches\n",
    "    for i in range(0, X_train.shape[0], BATCH_SIZE):\n",
    "        X_batch = X_train[i:i + BATCH_SIZE]\n",
    "        Y_batch = Y_train[i:i + BATCH_SIZE]\n",
    "        \n",
    "        # 1. FORWARD PASS\n",
    "        # Note: This forward_pass function needs to be adapted for batches \n",
    "        # (it currently only handles a single image).\n",
    "        # We'll use the first image for demonstration:\n",
    "        probabilities, cache = forward_pass(X_train[0], params)\n",
    "        \n",
    "        # 2. LOSS CALCULATION\n",
    "        loss = cross_entropy_loss(probabilities.reshape(1, -1), np.array([Y_train[0]]))\n",
    "        \n",
    "        # 3. BACKWARD PASS (The complex manual step we didn't code)\n",
    "        # gradients = manual_backward_pass(loss, cache, params)\n",
    "        \n",
    "        # 4. GRADIENT DESCENT (Optimization)\n",
    "        # for key in params:\n",
    "        #     params[key] -= LEARNING_RATE * gradients[key]\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
